{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fraud Consumer metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.shell import spark\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"MAST30034 Project 2\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_fraud_sdf = spark.read.parquet(\"../data/temp/consumer_fraud\")\n",
    "transaction_sdf1 = spark.read.parquet(\"../data/tables/transactions_20210228_20210827_snapshot\")\n",
    "transaction_sdf2 = spark.read.parquet(\"../data/tables/transactions_20210828_20220227_snapshot\")\n",
    "transaction_sdf3 = spark.read.parquet(\"../data/tables/transactions_20220228_20220828_snapshot\")\n",
    "transaction = transaction_sdf1.union(transaction_sdf2).union(transaction_sdf3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_sdf1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot on transaction 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "consumer_agg_daily_sdf1 = transaction_sdf1.groupBy([\"user_id\",\"order_datetime\"]).agg(\n",
    "    F.sum(F.col('dollar_value')).alias('total_amount'),\n",
    "    F.countDistinct(F.col('order_id')).alias(\"total_order\")\n",
    ")\n",
    "\n",
    "consumer_fraud_join_df1 = consumer_agg_daily_sdf1.join(consumer_fraud_sdf,on=[\"user_id\",\"order_datetime\"]).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(x=consumer_fraud_join_df1[\"total_amount\"],y=consumer_fraud_join_df1[\"fraud_probability\"])\n",
    "plt.xlabel('total_amount')\n",
    "plt.ylabel('fraud_probability') \n",
    "plt.title('total_amount vs fraud_probability')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=consumer_fraud_join_df1[\"total_order\"],y=consumer_fraud_join_df1[\"fraud_probability\"])\n",
    "plt.xlabel('total_order')\n",
    "plt.ylabel('fraud_probability') \n",
    "plt.title('total_order vs fraud_probability')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot on transaction 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_agg_daily_sdf2 = transaction_sdf2.groupBy([\"user_id\",\"order_datetime\"]).agg(\n",
    "    F.sum(F.col('dollar_value')).alias('total_amount'),\n",
    "    F.countDistinct(F.col('order_id')).alias(\"total_order\")\n",
    ")\n",
    "\n",
    "consumer_fraud_join_df2 = consumer_agg_daily_sdf2.join(consumer_fraud_sdf,on=[\"user_id\",\"order_datetime\"]).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=consumer_fraud_join_df2[\"total_amount\"],y=consumer_fraud_join_df2[\"fraud_probability\"])\n",
    "plt.xlabel('total_amount')\n",
    "plt.ylabel('fraud_probability') \n",
    "plt.title('total_amount vs fraud_probability')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=consumer_fraud_join_df2[\"total_order\"],y=consumer_fraud_join_df2[\"fraud_probability\"])\n",
    "plt.xlabel('total_order')\n",
    "plt.ylabel('fraud_probability') \n",
    "plt.title('total_order vs fraud_probability')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check distribution of normal transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_agg_daily_df1 = consumer_agg_daily_sdf1.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "\n",
    "ax1.scatter(x=consumer_agg_daily_df1['total_amount'], y=[0]*consumer_agg_daily_df1['total_amount'].count(),label=\"transaction 1\")\n",
    "ax1.scatter(x=consumer_fraud_join_df2[\"total_amount\"],y=consumer_fraud_join_df2[\"fraud_probability\"],label=\"fraud transaction\")\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# plt.scatter(x=consumer_agg_daily_df1['total_amount'], y=[np.mean(consumer_agg_daily_df1['total_amount'])]*consumer_agg_daily_df1['total_amount'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "\n",
    "ax1.hist(x=consumer_agg_daily_df1['total_amount'],label=\"transaction 1\")\n",
    "ax1.hist(x=consumer_fraud_join_df2[\"total_amount\"],label=\"fraud transaction\")\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression on fraud probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.formula.api import ols, glm\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "consumer_fraud_join_df_1_2 = consumer_fraud_join_df1.append(consumer_fraud_join_df2, ignore_index=True)\n",
    "\n",
    "fit = ols(\n",
    "    formula=\"fraud_probability ~ total_amount + total_order + total_amount/total_order\",\n",
    "    data=consumer_fraud_join_df_1_2\n",
    ").fit()\n",
    "\n",
    "print(fit.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "total_order is no longer significant, but the interaction term is.\n",
    "the interaction term explains why the scatter plot has a log like shape. \n",
    "When total amount is the same, less avg dollar value meaning less fraud prob\n",
    "\n",
    "### Formula: fraud_probability ~ total_amount + total_amount/total_order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model with pyspark model\n",
    " consider scalability we will use pyspark module instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_agg_daily_train = spark.createDataFrame(consumer_fraud_join_df_1_2).withColumn(\n",
    "    \"avg_dollar_value_per_order\",\n",
    "    F.col(\"total_amount\")/F.col(\"total_order\") # interaction feature added\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import Interaction\n",
    "# consumer_regression = consumer_agg_daily_all[[]].apply(lambda row:join_prob_p_val(row[0],row[1],row[2],row[3],row[4],row[5]),axis=1)\n",
    "features = 'features'\n",
    "input_cols = ['total_amount','avg_dollar_value_per_order'] \n",
    "# assembler = Interaction()\n",
    "# assembler.setInputCols([\"total_amount\", \"total_order\"])\n",
    "# assembler.setOutputCol(\"interaction\")\n",
    "assembler = VectorAssembler(\n",
    "    # which column to combine\n",
    "    inputCols=input_cols, \n",
    "    # How should the combined columns be named\n",
    "    outputCol=features\n",
    ")\n",
    "\n",
    "model_sdf = assembler.transform(consumer_agg_daily_train.dropna('any'))\n",
    "# Display the features and targets for our model\n",
    "model_sdf.select('features').head(5), model_sdf.select('fraud_probability').head(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LinearRegression(\n",
    "    featuresCol='features', \n",
    "    labelCol='fraud_probability',\n",
    "    maxIter=1000\n",
    ").fit(model_sdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lm.coefficients)\n",
    "print(lm.intercept)\n",
    "# coefficient is different, investigate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "\n",
    "model_path = \"../models\" + \"/lr_model\"\n",
    "lm.save(model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## apply the pyspark model to all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_agg_daily_all = transaction.groupBy([\"user_id\",\"order_datetime\"]).agg(\n",
    "    F.sum(F.col('dollar_value')).alias('total_amount'),\n",
    "    F.countDistinct(F.col('order_id')).alias(\"total_order\")\n",
    ").withColumn(\n",
    "    \"avg_dollar_value_per_order\",\n",
    "    F.col(\"total_amount\")/F.col(\"total_order\") # interaction feature added\n",
    ")\n",
    "#consumer_agg_daily_all.count() # 9 mil data\n",
    "consumer_agg_daily_all.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegressionModel\n",
    "model_path = \"../models\" + \"/lr_model\"\n",
    "lm = LinearRegressionModel.load(model_path)\n",
    "features = 'features'\n",
    "input_cols = ['total_amount','avg_dollar_value_per_order'] \n",
    "# assembler = Interaction()\n",
    "# assembler.setInputCols([\"total_amount\", \"total_order\"])\n",
    "# assembler.setOutputCol(\"interaction\")\n",
    "assembler = VectorAssembler(\n",
    "    # which column to combine\n",
    "    inputCols=input_cols, \n",
    "    # How should the combined columns be named\n",
    "    outputCol=features\n",
    ")\n",
    "\n",
    "predict_sdf = assembler.transform(consumer_agg_daily_all).select(features)\n",
    "result_sdf = lm.transform(predict_sdf)\n",
    "result_sdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_datetime_predict_sdf = consumer_agg_daily_all.join(result_sdf.select(\"prediction\"))\n",
    "user_datetime_predict_sdf= user_datetime_predict_sdf.withColumnRenamed(\n",
    "    \"prediction\",\n",
    "    \"fraud_prob\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_datetime_predict_sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cant save it\n",
    "#user_datetime_predict_sdf.write.mode('overwrite').parquet('../data/curated/user_datetime_predict_sdf.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#user_datetime_predict_sdf = spark.read.parquet(\"../data/curated/user_datetime_predict_sdf.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calculate the discounted revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, array\n",
    "from pyspark.sql.types import FloatType\n",
    "THRESHHOLD = lm.intercept\n",
    "\n",
    "def calculate_discounted_revenue(rate):\n",
    "    if rate < THRESHHOLD:\n",
    "        return 1\n",
    "    else: \n",
    "        return 1-rate\n",
    "       \n",
    "# sdf = sdf.withColumn(\n",
    "#     'transformed_col',\n",
    "#     some_udf(F.col('raw_col'))\n",
    "# )\n",
    "convert_rate = udf(lambda z: calculate_discounted_revenue(z),FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run slice by slice\n",
    "merchant_user_agg_sdf = transaction_sdf1.groupBy([\"merchant_abn\",\"order_datetime\",\"user_id\"]).agg(\n",
    "    F.countDistinct(F.col(\"order_id\")).alias(\"no_order\"),\n",
    "    F.sum(F.col(\"dollar_value\")).alias(\"dollar_amount\")\n",
    ")\n",
    "\n",
    "user_discounted_spending_sdf = merchant_user_agg_sdf \\\n",
    "    .join(user_datetime_predict_sdf,on=[\"user_id\",\"order_datetime\"]) \\\n",
    "    .withColumn(\n",
    "        \"convert_rate\",\n",
    "        convert_rate(F.col(\"fraud_prob\"))\n",
    "    ).withColumn(\n",
    "        \"discounted_spending\",\n",
    "        F.col(\"convert_rate\") * F.col(\"dollar_amount\")\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "merchant_discounted_daily_revenue = user_discounted_spending_sdf.groupBy([\"merchant_abn\",\"order_datetime\"]).agg(\n",
    "    F.sum(F.col(\"discounted_spending\")).alias(\"discounted_daily_revenue\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merchant_discounted_daily_revenue.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 31:> (0 + 0) / 200][Stage 32:(0 + 10) / 40000][Stage 35:>  (0 + 0) / 30]\r"
     ]
    }
   ],
   "source": [
    "merchant_discounted_daily_revenue.head() # cant run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Fraud data from the transaction set(discarded)\n",
    "since fraud data is only a small subset of the data, consider remove all transaction entries with fraud probability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert merchant fraud, consumer fraud to spark df\n",
    "merchant_fraud_df = pd.read_csv(\"../data/tables/merchant_fraud_probability.csv\")\n",
    "merchant_fraud_df.to_parquet(\"../data/temp/merchant_fraud\")\n",
    "merchant_fraud_sdf = spark.read.parquet(\"../data/temp/merchant_fraud\")\n",
    "\n",
    "consumer_fraud_df = pd.read_csv(\"../data/tables/consumer_fraud_probability.csv\")\n",
    "consumer_fraud_df.to_parquet(\"../data/temp/consumer_fraud\")\n",
    "consumer_fraud_sdf = spark.read.parquet(\"../data/temp/consumer_fraud\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save all transaction data to a spark dataframe: transaction\n",
    "transaction_sdf1 = spark.read.parquet(\"../data/tables/transactions_20210228_20210827_snapshot\")\n",
    "transaction_sdf2 = spark.read.parquet(\"../data/tables/transactions_20210828_20220227_snapshot\")\n",
    "transaction_sdf3 = spark.read.parquet(\"../data/tables/transactions_20220228_20220828_snapshot\")\n",
    "transaction = transaction_sdf1.union(transaction_sdf2).union(transaction_sdf3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all transaction on merchant fraud day\n",
    "merchant_fraud_sdf = spark.read.parquet(\"../data/temp/merchant_fraud\")\n",
    "merchant_transaction_on_fraud_day = transaction.join(merchant_fraud_sdf.select([\"merchant_abn\",\"order_datetime\"]), on=[\"merchant_abn\",\"order_datetime\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merchant_transaction_on_fraud_day.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all transaction on consumer fraud day\n",
    "consumer_fraud_sdf = spark.read.parquet(\"../data/temp/consumer_fraud\")\n",
    "consumer_transaction_on_fraud_day = transaction.join(consumer_fraud_sdf.select([\"user_id\",\"order_datetime\"]), on=[\"user_id\",\"order_datetime\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter fraud transactions\n",
    "\n",
    "order = [\"user_id\",\"merchant_abn\",\"dollar_value\",\"order_id\",\"order_datetime\"]\n",
    "transaction_fraud_rm = transaction.subtract(merchant_transaction_on_fraud_day.select(order)).subtract(consumer_transaction_on_fraud_day.select(order))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transaction_fraud_rm can be used for further analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derive fraud rate and other features\n",
    "\n",
    "#### Definition:\n",
    "\n",
    "Merchant Fraud rate: weighted fraud orders / total orders\n",
    "Fraud \n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6a51a4850383970a9d31be9f5e2bfc463bf284d7bc14ecdb113d3b740d850690"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ML')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
