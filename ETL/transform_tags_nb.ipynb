{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.shell import spark\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"MAST30034 Project 2\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True)\n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .getOrCreate()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4026"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf = spark.read.parquet(\"../data/tables/tbl_merchants.parquet\")\n",
    "# sdf.printSchema\n",
    "sdf.distinct().count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to pandas dataframe for convenience\n",
    "df = sdf.toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def extract_tags(arr, category=\"tags\"):\n",
    "\n",
    "    # Split tags into the three components\n",
    "    arr = arr[1:-1]\n",
    "    split_arr = re.split(\"\\), \\(|\\], \\[\", arr.strip(\"[()]\"))\n",
    "\n",
    "    if category == \"take_rate\":\n",
    "        return re.findall(\"[\\d\\.\\d]+\", split_arr[2])[0]\n",
    "\n",
    "    elif category == \"revenue_level\":\n",
    "        return split_arr[1].lower()\n",
    "\n",
    "    return split_arr[0].lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"tag\"] = df.apply(lambda row: extract_tags(row.tags, \"tags\"), axis=1)\n",
    "df[\"revenue_lvl\"] = df.apply(\n",
    "    lambda row: extract_tags(row.tags, \"revenue_level\"), axis=1\n",
    ")\n",
    "df[\"take_rate\"] = df.apply(lambda row: extract_tags(row.tags, \"take_rate\"), axis=1)\n",
    "\n",
    "# df.head(1)\n",
    "tag_col = df[\"tag\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /Users/ke/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# preprocess the data, with the following pipeline\n",
    "from nltk.metrics.distance import jaccard_distance\n",
    "from nltk.metrics.distance import edit_distance\n",
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import words\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk \n",
    "nltk.download('words')\n",
    "from autocorrect import Speller\n",
    "\n",
    "# use a class notion so only need to init corpus words and lemmatizer once\n",
    "# API: preprocess, which takes a single entry of tag, and returns preprocessed tag\n",
    "class Preprocessor:\n",
    "    def __init__(self, correct_method=\"auto\"):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.correct_words = words.words()\n",
    "        self.correct_method = correct_method\n",
    "        self.stopwords = set(stopwords.words('english'))\n",
    "\n",
    "        if correct_method == \"auto\":\n",
    "            self.auto_corrector = Speller(lang='en')\n",
    "    \n",
    "    # jaccard distance is better for minor typos\n",
    "    def __correct_spelling__(self,word):\n",
    "        # match the first character\n",
    "        if self.correct_method == \"jaccard\":\n",
    "            similarity_list = [(jaccard_distance(set(ngrams(word, 2)),set(ngrams(w, 2))),w) for w in self.correct_words if w[0]==word[0]]\n",
    "            similarity_list = sorted(similarity_list, key = lambda val:val[0])\n",
    "            return similarity_list[0][1]\n",
    "        if self.correct_method == \"edit_distance\":\n",
    "            similarity_list = [(edit_distance(word,w),w) for w in self.correct_words if w[0]==word[0]]\n",
    "            similarity_list = sorted(similarity_list, key = lambda val:val[0])\n",
    "            return similarity_list[0][1]\n",
    "        else:\n",
    "            return self.auto_corrector(word)\n",
    "\n",
    "\n",
    "    # case standardization -> puncuation, number removal -> \n",
    "    # tokenize -> spelling correction -> lemmatization -> minimum length\n",
    "    def preprocess(self,tag):\n",
    "        #case standardization\n",
    "        tag = tag.lower()\n",
    "        #puncuation, number removal\n",
    "        tag= re.sub(r'[^a-zA-Z\\s]', '',tag)\n",
    "        #tokenize \n",
    "        tag = word_tokenize(tag)\n",
    "\n",
    "        #correct spelling\n",
    "        tag = [self.__correct_spelling__(word) for word in tag ]\n",
    "\n",
    "        #stop word removal\n",
    "        tag = [word for word in tag if word not in self.stopwords]\n",
    "        \n",
    "        #lemmatization\n",
    "        tag = [self.lemmatizer.lemmatize(word) for word in tag]\n",
    "\n",
    "        #minimum length of 2\n",
    "        tag = [word for word in tag if len(word) > 2]\n",
    "\n",
    "        #join the word\n",
    "        tag = \" \".join(word for word in tag)\n",
    "        return tag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = preprocessor()\n",
    "# p.__correct_spelling__(\"friuts\")\n",
    "# # p.preprocess(\"appleee ,\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = Preprocessor()\n",
    "for i in range(tag_col.size):\n",
    "    tag_col[i] = preprocessor.preprocess(tag_col[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_col.to_csv(\"../data/curated/tag_col_preprocessed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_category_array = tag_col.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retail category loaded\n"
     ]
    }
   ],
   "source": [
    "# load retail category\n",
    "import json\n",
    "\n",
    "try:\n",
    "    f = open('retail_category.json')\n",
    "    # try load retail category dictionary\n",
    "    category_dict = json.load(f)\n",
    "    print(\"retail category loaded\")\n",
    "except:\n",
    "    print(\"category file not loadeds\")\n",
    "    # if retail dictionary doesn't exist, build one\n",
    "    category_dict = {}\n",
    "    for i in range(tag_category_array.size):\n",
    "         category_dict[i] = tag_category_array[i]\n",
    "    with open('retail_category.json', 'w') as f:\n",
    "        json.dump(category_dict, f)\n",
    "    print(\"retail category created\")\n",
    "finally:\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       furniture home furnishing equipment shop manuf...\n",
       "1            cable satellite pay television radio service\n",
       "2                     jewelry watch clock silverware shop\n",
       "3                         watch clock jewelry repair shop\n",
       "4         music shop musical instrument piano sheet music\n",
       "                              ...                        \n",
       "4021                       optician optical good eyeglass\n",
       "4022                            book periodical newspaper\n",
       "4023                                            shoe shop\n",
       "4024                        motor vehicle supply new part\n",
       "4025                        motor vehicle supply new part\n",
       "Name: tag, Length: 4026, dtype: object"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "COS_THRESHHOLD = 0.5 # tune this \n",
    "\n",
    "# match the new tag to existing tag\n",
    "existing_category = set(category_dict.values())\n",
    "\n",
    "data_category = set(tag_category_array)\n",
    "\n",
    "potential_new_category_list = list(data_category.difference(existing_category))\n",
    "\n",
    "# similar category are treated as distinct values\n",
    "all_category = existing_category.union(data_category)\n",
    "potential_new_category = data_category.difference\n",
    "\n",
    "# create a tf-idf vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    stop_words=\"english\", max_df=0.5, min_df=1, ngram_range=(1, 1)\n",
    ")\n",
    "tfidf_vectorizer.fit(list(all_category))\n",
    "\n",
    "\n",
    "for new_category in potential_new_category_list:\n",
    "    add_flag = False\n",
    "    for old_category in existing_category:\n",
    "        tfidf_vec_new = tfidf_vectorizer.transform([new_category])\n",
    "        tfidf_vec_old = tfidf_vectorizer.transform([old_category])\n",
    "        # put the new category to the old one if they are similar\n",
    "        if cosine_similarity(tfidf_vec_new,tfidf_vec_old) >= COS_THRESHHOLD:\n",
    "            add_flag = True\n",
    "            break\n",
    "\n",
    "    if add_flag:\n",
    "        existing_category.add(new_category)\n",
    "        last_key = int(list(category_dict.keys())[-1])\n",
    "        category_dict[last_key+1] = new_category\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "dump() missing 1 required positional argument: 'fp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/vm/4b2l4xh10_71ktbck3tm2plr0000gn/T/ipykernel_44120/3601437439.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategory_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: dump() missing 1 required positional argument: 'fp'"
     ]
    }
   ],
   "source": [
    "with open('retail_category.json', 'w') as f:\n",
    "    json.dump(category_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize the category tag (to number)\n",
    "for i in range(tag_col.size):\n",
    "    tag = tag_col[i]\n",
    "    category_key = -1\n",
    "    for key,v in category_dict.items():\n",
    "        if tag == v:\n",
    "            category_key = key\n",
    "            break\n",
    "    assert category_key != -1\n",
    "    tag_col[i] = key\n",
    "\n",
    "tag_col.to_csv(\"../data/curated/tag_col_standardized.csv\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6a51a4850383970a9d31be9f5e2bfc463bf284d7bc14ecdb113d3b740d850690"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ML')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
