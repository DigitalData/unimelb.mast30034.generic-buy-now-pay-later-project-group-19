{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL - Tags Transformation\n",
    "\n",
    "### Importing Libraries and Loading Merchant Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.shell import spark\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"MAST30034 Project 2\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True)\n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .getOrCreate()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = spark.read.parquet(\"../data/tables/tbl_merchants.parquet\")\n",
    "sdf.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to pandas dataframe for convenience\n",
    "df = sdf.toPandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def extract_tags(arr, category=\"tags\"):\n",
    "\n",
    "    # Split tags into the three components\n",
    "    arr = arr[1:-1]\n",
    "    split_arr = re.split(\"\\), \\(|\\], \\[\", arr.strip(\"[()]\"))\n",
    "\n",
    "    if category == \"take_rate\":\n",
    "        return re.findall(\"[\\d\\.\\d]+\", split_arr[2])[0]\n",
    "\n",
    "    elif category == \"revenue_level\":\n",
    "        return split_arr[1].lower()\n",
    "\n",
    "    return split_arr[0].lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"tag\"] = df.apply(lambda row: extract_tags(row.tags, \"tags\"), axis=1)\n",
    "df[\"revenue_level\"] = df.apply(\n",
    "    lambda row: extract_tags(row.tags, \"revenue_level\"), axis=1\n",
    ")\n",
    "df[\"take_rate\"] = df.apply(lambda row: extract_tags(row.tags, \"take_rate\"), axis=1)\n",
    "\n",
    "tag_col = df[\"tag\"].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Texts in Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the data, with the following pipeline\n",
    "from nltk.metrics.distance import jaccard_distance\n",
    "from nltk.metrics.distance import edit_distance\n",
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import words\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk \n",
    "nltk.download('words')\n",
    "from autocorrect import Speller\n",
    "\n",
    "# use a class notion so only need to init corpus words and lemmatizer once\n",
    "# API: preprocess, which takes a single entry of tag, and returns preprocessed tag\n",
    "class Preprocessor:\n",
    "    def __init__(self, correct_method=\"auto\"):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.correct_words = words.words()\n",
    "        self.correct_method = correct_method\n",
    "        self.stopwords = set(stopwords.words('english'))\n",
    "\n",
    "        if correct_method == \"auto\":\n",
    "            self.auto_corrector = Speller(lang='en')\n",
    "    \n",
    "    # jaccard distance is better for minor typos\n",
    "    def __correct_spelling__(self,word):\n",
    "        # match the first character\n",
    "        if self.correct_method == \"jaccard\":\n",
    "            similarity_list = [(jaccard_distance(set(ngrams(word, 2)),set(ngrams(w, 2))),w) for w in self.correct_words if w[0]==word[0]]\n",
    "            similarity_list = sorted(similarity_list, key = lambda val:val[0])\n",
    "            return similarity_list[0][1]\n",
    "        if self.correct_method == \"edit_distance\":\n",
    "            similarity_list = [(edit_distance(word,w),w) for w in self.correct_words if w[0]==word[0]]\n",
    "            similarity_list = sorted(similarity_list, key = lambda val:val[0])\n",
    "            return similarity_list[0][1]\n",
    "        else:\n",
    "            return self.auto_corrector(word)\n",
    "\n",
    "\n",
    "    # case standardization -> puncuation, number removal -> \n",
    "    # tokenize -> spelling correction -> lemmatization -> minimum length\n",
    "    def preprocess(self,tag_line):\n",
    "        #case standardization\n",
    "        tag_line = tag_line.lower()\n",
    "        \n",
    "        #puncuation, number removal, except comma\n",
    "        tag_line= re.sub(r'[^a-zA-Z\\s,]', ' ',tag_line)\n",
    "        \n",
    "        #tokenize by comma\n",
    "        tag_line = tag_line.split(',')\n",
    "        \n",
    "        \n",
    "        #strip leading and ending of tag\n",
    "        tag_line = [text.strip() for text in tag_line]\n",
    "\n",
    "        new_tag_line = []\n",
    "        for tag in tag_line:\n",
    "\n",
    "            new_tag = word_tokenize(tag)\n",
    "\n",
    "            #correct spelling\n",
    "            new_tag = [self.__correct_spelling__(text) for text in new_tag ]\n",
    "        \n",
    "            #stop word removal\n",
    "            new_tag = [text for text in new_tag if text not in self.stopwords]\n",
    "\n",
    "            #lemmatization\n",
    "            new_tag = [self.lemmatizer.lemmatize(text) for text in new_tag]\n",
    "\n",
    "            #minimum length of 2\n",
    "            new_tag = \" \".join([text for text in new_tag if len(text) > 2])\n",
    "            \n",
    "            new_tag_line.append(new_tag)\n",
    "\n",
    "        return \",\".join(new_tag_line)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = Preprocessor()\n",
    "for i in range(tag_col.size):\n",
    "    tag_col[i] = preprocessor.preprocess(tag_col[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = Preprocessor()\n",
    "# p.preprocess(tag_col[0])\n",
    "# tag_col.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Bag-of-Words Method with Tags to Categorise Merchants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count and vectorize text in tags with comma as delimiter\n",
    "vectorizer = CountVectorizer(tokenizer=lambda text: re.split(',',text))\n",
    "X = vectorizer.fit_transform(tag_col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the vectorizer with merchant data\n",
    "count_vect_df = pd.DataFrame(X.todense(), columns=vectorizer.get_feature_names())\n",
    "merchant_category = pd.concat([df, count_vect_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check all categories only 58 unique tags\n",
    "# count_vect_df.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save preprocessed tag column\n",
    "tag_col.to_csv(\"../data/curated/tag_col_preprocessed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Remove the tags columns\n",
    "merchant_category.drop(['tags','tag'], axis=1, inplace=True)\n",
    "# save expanded tags dataframe\n",
    "merchant_category.to_csv(\"../data/curated/merchant_tag.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An example of analysis the data by tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find all merchant abns associated with each category/tag\n",
    "merchants_by_categories = {}\n",
    "for category in count_vect_df.columns:\n",
    "    merchants_by_categories[category]=merchant_category[merchant_category[category] != 0]['merchant_abn'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merchant_category[merchant_category['merchant_abn'].isin(merchants_by_categories['card'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An Alternative method of grouping tags (not used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load retail category\n",
    "# import json\n",
    "\n",
    "# try:\n",
    "#     f = open('retail_category.json')\n",
    "#     # try load retail category dictionary\n",
    "#     category_dict = json.load(f)\n",
    "#     print(\"retail category loaded\")\n",
    "# except:\n",
    "#     print(\"category file not loadeds\")\n",
    "#     # if retail dictionary doesn't exist, build one\n",
    "#     category_dict = {}\n",
    "#     for i in range(tag_category_array.size):\n",
    "#          category_dict[i] = tag_category_array[i]\n",
    "#     with open('retail_category.json', 'w') as f:\n",
    "#         json.dump(category_dict, f)\n",
    "#     print(\"retail category created\")\n",
    "# finally:\n",
    "#     f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# COS_THRESHHOLD = 0.5 # tune this \n",
    "\n",
    "# # match the new tag to existing tag\n",
    "# existing_category = set(category_dict.values())\n",
    "\n",
    "# data_category = set(tag_category_array)\n",
    "\n",
    "# potential_new_category_list = list(data_category.difference(existing_category))\n",
    "\n",
    "# # similar category are treated as distinct values\n",
    "# all_category = existing_category.union(data_category)\n",
    "# potential_new_category = data_category.difference\n",
    "\n",
    "# # create a tf-idf vectorizer\n",
    "# tfidf_vectorizer = TfidfVectorizer(\n",
    "#     stop_words=\"english\", max_df=0.5, min_df=1, ngram_range=(1, 1)\n",
    "# )\n",
    "# tfidf_vectorizer.fit(list(all_category))\n",
    "\n",
    "\n",
    "# for new_category in potential_new_category_list:\n",
    "#     add_flag = False\n",
    "#     for old_category in existing_category:\n",
    "#         tfidf_vec_new = tfidf_vectorizer.transform([new_category])\n",
    "#         tfidf_vec_old = tfidf_vectorizer.transform([old_category])\n",
    "#         # put the new category to the old one if they are similar\n",
    "#         if cosine_similarity(tfidf_vec_new,tfidf_vec_old) >= COS_THRESHHOLD:\n",
    "#             add_flag = True\n",
    "#             break\n",
    "\n",
    "#     if add_flag:\n",
    "#         existing_category.add(new_category)\n",
    "#         last_key = int(list(category_dict.keys())[-1])\n",
    "#         category_dict[last_key+1] = new_category\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(all_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('retail_category.json', 'w') as f:\n",
    "#     json.dump(category_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # standardize the category tag (to number)\n",
    "# for i in range(tag_col.size):\n",
    "#     tag = tag_col[i]\n",
    "#     category_key = -1\n",
    "#     for key,v in category_dict.items():\n",
    "#         if tag == v:\n",
    "#             category_key = key\n",
    "#             break\n",
    "#     assert category_key != -1\n",
    "#     tag_col[i] = key\n",
    "\n",
    "# tag_col.to_csv(\"../data/curated/tag_col_standardized.csv\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6a51a4850383970a9d31be9f5e2bfc463bf284d7bc14ecdb113d3b740d850690"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
