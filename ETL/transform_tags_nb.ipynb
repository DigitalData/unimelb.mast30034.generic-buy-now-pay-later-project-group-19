{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/09/01 18:27:45 WARN Utils: Your hostname, Kes-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 10.13.225.200 instead (on interface en0)\n",
      "22/09/01 18:27:45 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/ke/opt/anaconda3/envs/ML/lib/python3.9/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/09/01 18:27:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/09/01 18:27:46 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/09/01 18:27:46 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "22/09/01 18:27:46 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 3.1.2\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.9.7 (default, Sep 16 2021 08:50:36)\n",
      "Spark context Web UI available at http://10.13.225.200:4043\n",
      "Spark context available as 'sc' (master = local[*], app id = local-1662020866826).\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.shell import spark\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"MAST30034 Project 2\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True)\n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .getOrCreate()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4026"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf = spark.read.parquet(\"../data/tables/tbl_merchants.parquet\")\n",
    "# sdf.printSchema\n",
    "sdf.distinct().count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to pandas dataframe for convenience\n",
    "df = sdf.toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def extract_tags(arr, category=\"tags\"):\n",
    "\n",
    "    # Split tags into the three components\n",
    "    arr = arr[1:-1]\n",
    "    split_arr = re.split(\"\\), \\(|\\], \\[\", arr.strip(\"[()]\"))\n",
    "\n",
    "    if category == \"take_rate\":\n",
    "        return re.findall(\"[\\d\\.\\d]+\", split_arr[2])[0]\n",
    "\n",
    "    elif category == \"revenue_level\":\n",
    "        return split_arr[1].lower()\n",
    "\n",
    "    return split_arr[0].lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"tag\"] = df.apply(lambda row: extract_tags(row.tags, \"tags\"), axis=1)\n",
    "df[\"revenue_lvl\"] = df.apply(\n",
    "    lambda row: extract_tags(row.tags, \"revenue_level\"), axis=1\n",
    ")\n",
    "df[\"take_rate\"] = df.apply(lambda row: extract_tags(row.tags, \"take_rate\"), axis=1)\n",
    "\n",
    "# df.head(1)\n",
    "tag_col = df[\"tag\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /Users/ke/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# preprocess the data, with the following pipeline\n",
    "from nltk.metrics.distance import jaccard_distance\n",
    "from nltk.metrics.distance import edit_distance\n",
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import words\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk \n",
    "nltk.download('words')\n",
    "from autocorrect import Speller\n",
    "\n",
    "# use a class notion so only need to init corpus words and lemmatizer once\n",
    "# API: preprocess, which takes a single entry of tag, and returns preprocessed tag\n",
    "class Preprocessor:\n",
    "    def __init__(self, correct_method=\"auto\"):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.correct_words = words.words()\n",
    "        self.correct_method = correct_method\n",
    "\n",
    "        if correct_method == \"auto\":\n",
    "            self.auto_corrector = Speller(lang='en')\n",
    "    \n",
    "    # jaccard distance is better for minor typos\n",
    "    def __correct_spelling__(self,word):\n",
    "        # match the first character\n",
    "        if self.correct_method == \"jaccard\":\n",
    "            similarity_list = [(jaccard_distance(set(ngrams(word, 2)),set(ngrams(w, 2))),w) for w in self.correct_words if w[0]==word[0]]\n",
    "            similarity_list = sorted(similarity_list, key = lambda val:val[0])\n",
    "            return similarity_list[0][1]\n",
    "        if self.correct_method == \"edit_distance\":\n",
    "            similarity_list = [(edit_distance(word,w),w) for w in self.correct_words if w[0]==word[0]]\n",
    "            similarity_list = sorted(similarity_list, key = lambda val:val[0])\n",
    "            return similarity_list[0][1]\n",
    "        else:\n",
    "            return self.auto_corrector(word)\n",
    "\n",
    "\n",
    "    # case standardization -> puncuation, number removal -> \n",
    "    # tokenize -> spelling correction -> lemmatization -> minimum length\n",
    "    def preprocess(self,tag):\n",
    "        #case standardization\n",
    "        tag = tag.lower()\n",
    "        #puncuation, number removal\n",
    "        tag= re.sub(r'[^a-zA-Z\\s]', '',tag)\n",
    "        #tokenize \n",
    "        tag = word_tokenize(tag)\n",
    "\n",
    "        #correct spelling\n",
    "        tag = [self.__correct_spelling__(word) for word in tag ]\n",
    "        #lemmatization\n",
    "        # tag = [self.lemmatizer.lemmatize(word) for word in tag]\n",
    "\n",
    "        #minimum length of 2\n",
    "        tag = [word for word in tag if len(word) > 2]\n",
    "\n",
    "        #join the word\n",
    "        tag = \" \".join(word for word in tag)\n",
    "        return tag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fruits'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# p = preprocessor()\n",
    "# p.__correct_spelling__(\"friuts\")\n",
    "# # p.preprocess(\"appleee ,\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = Preprocessor()\n",
    "for i in range(tag_col.size):\n",
    "    tag_col[i] = preprocessor.preprocess(tag_col[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_col.to_csv(\"../data/curated/tag_col_preprocessed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       furniturehomefurnishingsandequipmentshopsandma...\n",
       "1       cablesatelliteandotherpaytelevisionandradioser...\n",
       "2                     jewelrywatchclockandsilverwareshops\n",
       "3                         watchclockandjewelryrepairshops\n",
       "4         musicshopsmusicalinstrumentspianosandsheetmusic\n",
       "                              ...                        \n",
       "4021                   opticiansopticalgoodsandeyeglasses\n",
       "4022                        booksperiodicalsandnewspapers\n",
       "4023                                            shoeshops\n",
       "4024                      motorvehiclesuppliesandnewparts\n",
       "4025                      motorvehiclesuppliesandnewparts\n",
       "Name: tag, Length: 4026, dtype: object"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_tfidf_vectorizer = TfidfVectorizer(\n",
    "    stop_words=\"english\", max_df=0.5, min_df=1, ngram_range=(1, 1), max_features=100000\n",
    ")\n",
    "unigram_tfidf_vectorizer.fit_transform(tag_col)\n",
    "\n",
    "tag_col\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6a51a4850383970a9d31be9f5e2bfc463bf284d7bc14ecdb113d3b740d850690"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ML')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
